{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6f7b8-14ed-42d3-8a88-0306b496fd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-02-06 10:00:18,472 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,472 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,482 [RapidOCR] download_file.py:60: File exists and is valid: D:\\Git\\docling_demo\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,482 [RapidOCR] main.py:50: Using D:\\Git\\docling_demo\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,472 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,482 [RapidOCR] download_file.py:60: File exists and is valid: D:\\Git\\docling_demo\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,482 [RapidOCR] main.py:50: Using D:\\Git\\docling_demo\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,571 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,572 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,574 [RapidOCR] download_file.py:60: File exists and is valid: D:\\Git\\docling_demo\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,574 [RapidOCR] main.py:50: Using D:\\Git\\docling_demo\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,891 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,891 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,907 [RapidOCR] download_file.py:60: File exists and is valid: D:\\Git\\docling_demo\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-06 10:00:18,908 [RapidOCR] main.py:50: Using D:\\Git\\docling_demo\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# Initialize converter with default settings\n",
    "converter = DocumentConverter()\n",
    "\n",
    "# Convert the document into structured data\n",
    "source_url = \"https://arxiv.org/pdf/2408.09869\"\n",
    "result = converter.convert(source_url)\n",
    "\n",
    "# Access structured data immediately\n",
    "doc = result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77ca1b-896e-4e85-bea0-cafa4b3f2696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HierarchicalChunker: 77 chunks\n",
      "Chunk length: 295 characters\n",
      "Chunk content: Christoph Auer Maksym Lysak Ah... Kuropiatnyk Peter W. J. Staar\n",
      "--------------------------------------------------\n",
      "Chunk length: 50 characters\n",
      "Chunk content: AI4K Group, IBM Research R¨ us...arch R¨ uschlikon, Switzerland\n",
      "--------------------------------------------------\n",
      "Chunk length: 431 characters\n",
      "Chunk content: This technical report introduc...on of new features and models.\n",
      "--------------------------------------------------\n",
      "Chunk length: 792 characters\n",
      "Chunk content: Converting PDF documents back ... gap to proprietary solutions.\n",
      "--------------------------------------------------\n",
      "Chunk length: 488 characters\n",
      "Chunk content: With Docling , we open-source ...on of new features and models.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from docling.chunking import HierarchicalChunker\n",
    "def print_chunk(chunk):\n",
    "    print(f\"Chunk length: {len(chunk.text)} characters\")\n",
    "    if len(chunk.text) > 30:\n",
    "        print(f\"Chunk content: {chunk.text[:30]}...{chunk.text[-30:]}\")\n",
    "    else:\n",
    "        print(f\"Chunk content: {chunk.text}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "\n",
    "# Process with HierarchicalChunker (structure-based)\n",
    "hierarchical_chunker = HierarchicalChunker()\n",
    "hierarchical_chunks = list(hierarchical_chunker.chunk(doc))\n",
    "\n",
    "print(f\"HierarchicalChunker: {len(hierarchical_chunks)} chunks\")\n",
    "\n",
    "# Print the first 3 chunks\n",
    "for chunk in hierarchical_chunks[:5]:\n",
    "    print_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc76a5a-e98d-4c95-a189-a9f3b64c90f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridChunker: 24 chunks\n",
      "Chunk length: 346 characters\n",
      "Chunk content: Christoph Auer Maksym Lysak Ah...arch R¨ uschlikon, Switzerland\n",
      "--------------------------------------------------\n",
      "Chunk length: 431 characters\n",
      "Chunk content: This technical report introduc...on of new features and models.\n",
      "--------------------------------------------------\n",
      "Chunk length: 1846 characters\n",
      "Chunk content: Converting PDF documents back ... accelerators (GPU, MPS, etc).\n",
      "--------------------------------------------------\n",
      "Chunk length: 1428 characters\n",
      "Chunk content: To use Docling, you can simply...and run it inside a container.\n",
      "--------------------------------------------------\n",
      "Chunk length: 796 characters\n",
      "Chunk content: Docling implements a linear pi...erialized to JSON or Markdown.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "\n",
    "# Process with HybridChunker (token-aware)\n",
    "hybrid_chunker = HybridChunker(max_tokens=1024, overlap_tokens=50)\n",
    "hybrid_chunks = list(hybrid_chunker.chunk(doc))\n",
    "\n",
    "print(f\"HybridChunker: {len(hybrid_chunks)} chunks\")\n",
    "\n",
    "# Print the first 3 chunks\n",
    "for chunk in hybrid_chunks[:5]:\n",
    "    print_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b22ae-45e4-47b7-9097-57cd441158e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 30 intelligent chunks\n",
      "Built vector store with 30 chunks\n"
     ]
    }
   ],
   "source": [
    "## See https://codecut.ai/semantic-search-postgres-pgvector-ollama/ for postgres vector chunking\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "# Initialize the chunker\n",
    "chunker = HybridChunker(max_tokens=512, overlap_tokens=50)\n",
    "\n",
    "# Create the chunks\n",
    "rag_chunks = list(chunker.chunk(doc))\n",
    "\n",
    "print(f\"Created {len(rag_chunks)} intelligent chunks\")\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the vector store\n",
    "texts = [chunk.text for chunk in rag_chunks]\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "print(f\"Built vector store with {len(texts)} chunks\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
